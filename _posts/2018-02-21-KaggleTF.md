---
layout: post
title: Kaggle competition - TF speech recognition challenge
author: Author Heming
---

It is rare to see a speech and audio processing competition on Kaggle. Though the competition was already closed a month ago. I thought it is still worthwhile to dig into it a little bit, to see some practical engineer tricks.

The competition is not the typical 'ASR' problem. Instead, we are only asked to recognize 12 words. This tiny dictionary makes our task a pure classification problem. Or rather called 'small footprint'.

My first initial attempt was somewhat 'naive'. I just simply applied two transforms to the signal: pad_transform (to ensure audio signals have the same duration), and log scale melspectrum. Pass the data to a 2-layer CNN, I obtained around 72% accuracy on the test dataset.

Then I follow some of the discussion and kernels to reach around 85%. Still not that encouraging. But good enough for me, since my goal is not to beat this competition, just to learn others' method. Some of the solutions are excellent, but require huge amount of coding. I'd rather just read through their projects :)

The first place solution did an amazing job to process the raw data. He first 'pseudo labeled' the silence and 'unknown' classes, by builting his own pytorch sampler. He obtained good distribution for the 'unknown samples'. Then he also trained on mel-spectrum (combined with MFCC features). And of course, he used the ensemble to combine several models.


Second place solution: used 20 log-mel filterbanks to extract features. The nn is the normal 2D CNN, but the author managed to considerably augument the speech data. First of all, normalize the windowed peak value to obtain similar scale for different speech pieces. Secondly, Vocal Tract Length Perturbation (VLTP), using a random linear warping along the frequency dimension. Taking average the prediction over multiple warp factors.

Ref: https://pdfs.semanticscholar.org/3de0/616eb3cd4554fdf9fd65c9c82f2605a17413.pdf

Thrid place solution utlized some higher-level networks, including ResNet, Senet, Densenet, VGG. Then applied the semi-supervised learning.


Take away message:
1. Understand the data before training
2. For speech data, the most effective feature seems to be related to auditary mel-spectrum. 2D spectrum performs better than 1D MFCC.
3. Speech augumentation is somewhat different from image. We cannot simply rotate and strectch. Need to do relevant transform to enrich acoustic information.
4. When extracting spectrum, need to retain some time information by carefully choosing time frame.


